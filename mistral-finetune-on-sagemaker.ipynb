{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0d4eac",
   "metadata": {},
   "source": [
    "# How to fine-tune Mistral models on AWS Sagemaker\n",
    "\n",
    "This sample notebook explains how to fine-tune and deploy a custom Mistral model on AWS Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a475013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker as sage\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "\n",
    "from typing import (\n",
    "    Optional,\n",
    "    Dict,\n",
    "    Any,\n",
    "    List,\n",
    "    Union\n",
    ")\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.response import StreamingBody\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT VALUES (change only if/when needed)\n",
    "DEFAULT_TRAINING_INSTANCE_VOLUME_SIZE_GB = 200\n",
    "DEFAULT_TRAINING_MAX_RUNTIME_S = 3600\n",
    "\n",
    "\n",
    "def build_s3_uri(s3_bucket: str, s3_data_dir: str, object_path: str) -> str:\n",
    "    return \"s3://\" + str(Path(*[s3_bucket, s3_data_dir, object_path]))\n",
    "\n",
    "\n",
    "def generate_ts_str() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "\n",
    "def get_model_name_from_arn(arn: str) -> str:\n",
    "    pattern = r\"(mistral|codestral|pixtral|ministral)-\\d+b-\\d+\"\n",
    "    match = re.search(pattern, arn)\n",
    "    if match:\n",
    "        model_name = match.group(0)\n",
    "        return model_name\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract model name from resource {arn}!\")\n",
    "\n",
    "        \n",
    "def create_mistral_ft_job(job_params: Dict[str, Any]) -> str:\n",
    "    logger.info(\"Starting fine-tuning training job creation...\")\n",
    "    \n",
    "    # Unpack parameters\n",
    "    logger.info(\"Unpacking job_params...\")\n",
    "    try:\n",
    "        execution_role_arn = job_params[\"execution_role_arn\"]\n",
    "        algorithm_arn = job_params[\"algorithm_arn\"]\n",
    "        training_dataset = job_params[\"training_dataset\"]\n",
    "        validation_dataset = job_params[\"validation_dataset\"]\n",
    "        config_file = job_params[\"config_file\"]\n",
    "        lora_dir = job_params[\"lora_dir\"]\n",
    "        instance_type = job_params[\"instance_type\"]\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing/incorrect parameter: {e}\")\n",
    "    \n",
    "    ft_job_name = f\"{get_model_name_from_arn(algorithm_arn)}-ft-{generate_ts_str()}-training-job\"\n",
    "    logger.info(f\"Starting preparation for training job {ft_job_name}...\")\n",
    "    \n",
    "    # Input data configuration\n",
    "    input_data_config: List[Dict[str, Any]] = []\n",
    "    channels = [\n",
    "        {\n",
    "            \"name\": \"training_data\",\n",
    "            \"s3_uri\": training_dataset\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"validation_data\",\n",
    "            \"s3_uri\": validation_dataset\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"configuration\",\n",
    "            \"s3_uri\": config_file\n",
    "        }\n",
    "    ]\n",
    "    for channel in channels:\n",
    "        input_data_config.append({\n",
    "            \"ChannelName\": channel[\"name\"],\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": channel[\"s3_uri\"],\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        })\n",
    "    \n",
    "    # Output data configuration\n",
    "    output_data_config = {\n",
    "        \"S3OutputPath\": lora_dir,\n",
    "        \"CompressionType\": \"GZIP\"\n",
    "    }\n",
    "    resource_config = {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"VolumeSizeInGB\": DEFAULT_TRAINING_INSTANCE_VOLUME_SIZE_GB\n",
    "    }\n",
    "    stopping_condition = {\"MaxRuntimeInSeconds\": DEFAULT_TRAINING_MAX_RUNTIME_S}\n",
    "\n",
    "    # API call for training job startup\n",
    "    logger.info(f\"Launching training job {ft_job_name}...\")\n",
    "    try:\n",
    "        client = boto3.client(\"sagemaker\")\n",
    "        ft_job = client.create_training_job(\n",
    "            TrainingJobName=ft_job_name,\n",
    "            RoleArn=execution_role_arn,\n",
    "            AlgorithmSpecification={\"AlgorithmName\": algorithm_arn, \"TrainingInputMode\": \"File\"},\n",
    "            InputDataConfig=input_data_config,\n",
    "            OutputDataConfig=output_data_config,\n",
    "            ResourceConfig=resource_config,\n",
    "            StoppingCondition=stopping_condition\n",
    "        )\n",
    "        ft_job_arn = ft_job[\"TrainingJobArn\"]\n",
    "        logger.info(f\"Training job succesfully submitted (ARN: {ft_job_arn})\")\n",
    "        return ft_job_arn\n",
    "    except (\n",
    "        client.exceptions.ResourceInUse,\n",
    "        client.exceptions.ResourceLimitExceeded,\n",
    "        client.exceptions.ResourceNotFound\n",
    "    ) as e:\n",
    "        logger.error(f\"Sagemaker error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "    \n",
    "def get_mistral_ft_job_info(ft_job_arn) -> Dict[str, Any]:\n",
    "    logger.info(f\"Fetching information for training job {ft_job_arn}...\")\n",
    "    client = boto3.client(\"sagemaker\")\n",
    "    ft_job_name = ft_job_arn.split('/')[-1]\n",
    "    ft_job_info_raw = client.describe_training_job(TrainingJobName=ft_job_name)\n",
    "    status = ft_job_info_raw[\"TrainingJobStatus\"]\n",
    "    events = [event for event in ft_job_info_raw[\"SecondaryStatusTransitions\"]]\n",
    "    if status == \"Completed\":\n",
    "        model_artifact_uri = ft_job_info_raw[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "    else:\n",
    "        model_artifact_uri = None\n",
    "    algorithm_arn = ft_job_info_raw[\"AlgorithmSpecification\"][\"AlgorithmName\"]\n",
    "    ft_job_info = {\n",
    "        \"job_name\": ft_job_name,\n",
    "        \"status\": status,\n",
    "        \"events\": events,\n",
    "        \"model_artifact_uri\": model_artifact_uri,\n",
    "        \"algorithm_arn\": algorithm_arn\n",
    "    }\n",
    "    return ft_job_info\n",
    "\n",
    "\n",
    "def create_mistral_ft_model_package(ft_job_info: Dict[str, Any]) -> str:\n",
    "    logger.info(\"Starting model package creation...\")\n",
    "    package_name = ft_job_info[\"job_name\"].replace(\"training-job\", \"model-package\")\n",
    "    client = boto3.client(\"sagemaker\")\n",
    "    try:   \n",
    "        response = client.create_model_package(\n",
    "            ModelPackageName = package_name,\n",
    "            SourceAlgorithmSpecification={\n",
    "                \"SourceAlgorithms\": [\n",
    "                    {\n",
    "                        \"AlgorithmName\": ft_job_info[\"algorithm_arn\"],\n",
    "                        \"ModelDataUrl\": ft_job_info[\"model_artifact_uri\"]\n",
    "\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        model_package_arn = response[\"ModelPackageArn\"]\n",
    "        logger.info(f\"Model package creation successfully submitted (ARN: {model_package_arn})\")\n",
    "        return model_package_arn\n",
    "    except (\n",
    "        client.exceptions.ConflictException,\n",
    "        client.exceptions.ResourceLimitExceeded\n",
    "    ) as e:\n",
    "        logger.error(f\"Sagemaker error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    \n",
    "def create_mistral_ft_endpoint(model_package_arn: str, instance_type: str) -> str:\n",
    "    logger.info(f\"Starting model endpoint preparation from package {model_package_arn}...\")\n",
    "    client = boto3.client(\"sagemaker\")\n",
    "    model_package_status = client \\\n",
    "        .describe_model_package(ModelPackageName=model_package_arn)\\\n",
    "        .get(\"ModelPackageStatus\")\n",
    "    if model_package_status != \"Completed\":\n",
    "        raise ValueError(f\"Model package {model_package_arn} is not ready (status={model_package_status})\")\n",
    "    else:\n",
    "        # Create model\n",
    "        logger.info(\"Creating model...\")\n",
    "        try:\n",
    "            model_name = model_package_arn.split('/')[-1].replace(\"model-package\", \"model\")\n",
    "            model_resp = client.create_model(\n",
    "                ModelName=model_name,\n",
    "                ExecutionRoleArn=sage.get_execution_role(),\n",
    "                PrimaryContainer={\"ModelPackageName\": model_package_arn}\n",
    "            )\n",
    "            logger.info(f\"Model successfully created (model_name: {model_name})\")\n",
    "        except client.exceptions.ResourceLimitExceeded as e:\n",
    "            logger.error(f\"Sagemaker error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unknown error: {e}\")\n",
    "            raise\n",
    "        # Create endpoint configuration\n",
    "        logger.info(\"Creating endpoint configuration...\")\n",
    "        endpoint_config_name = f\"{model_name}-config\"\n",
    "        endpoint_config = {\n",
    "            \"EndpointConfigName\": endpoint_config_name,\n",
    "            \"ProductionVariants\": [\n",
    "                {\n",
    "                    \"VariantName\": \"model\",\n",
    "                    \"ModelName\": model_name,\n",
    "                    \"InstanceType\": instance_type,\n",
    "                    \"InitialInstanceCount\": 1,\n",
    "                    \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "                    \"ModelDataDownloadTimeoutInSeconds\": 3600\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        try:\n",
    "            endpoint_config_resp = client.create_endpoint_config(**endpoint_config)\n",
    "            endpoint_config_arn = endpoint_config_resp[\"EndpointConfigArn\"]\n",
    "            logger.info(f\"Endpoint configuration successfully created (ARN: {endpoint_config_arn})\")\n",
    "        except client.exceptions.ResourceLimitExceeded as e:\n",
    "            logger.error(f\"Sagemaker error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unknown error: {e}\")\n",
    "            raise\n",
    "        # Create endpoint\n",
    "        logger.info(\"Creating endpoint...\")\n",
    "        try:\n",
    "            endpoint_resp = client.create_endpoint(\n",
    "                EndpointName=model_name,\n",
    "                EndpointConfigName=endpoint_config_name\n",
    "            )\n",
    "            endpoint_arn = endpoint_resp[\"EndpointArn\"]\n",
    "            logger.info(f\"Endpoint successfully created (ARN: f{endpoint_arn})\")\n",
    "            return endpoint_arn\n",
    "        except client.exceptions.ResourceLimitExceeded as e:\n",
    "            logger.error(f\"Sagemaker error: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unknown error: {e}\")\n",
    "            \n",
    "            \n",
    "def query_mistral_ft_endpoint(endpoint_arn: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "    client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    logger.info(f\"Checking endpoint status (ARN: {endpoint_arn})\")\n",
    "    endpoint_name = endpoint_arn.split('/')[-1]\n",
    "    endpoint_status = client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if endpoint_status == \"InService\":\n",
    "        logger.info(\"Endpoint is running. Sending inference request...\")\n",
    "        try:\n",
    "            inference_out = runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType=\"application/json\",\n",
    "                Body=payload\n",
    "            )\n",
    "            #resp_bytes = inference_out[\"Body\"]\n",
    "            #return resp_bytes\n",
    "            inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "            return json.loads(inference_resp_str)\n",
    "        except (\n",
    "            runtime_client.exceptions.InternalFailure,\n",
    "            runtime_client.exceptions.ServiceUnavailable,\n",
    "            runtime_client.exceptions.ValidationError,\n",
    "            runtime_client.exceptions.ModelError,\n",
    "            runtime_client.exceptions.InternalDependencyException,\n",
    "            runtime_client.exceptions.ModelNotReadyException\n",
    "        ) as e:\n",
    "            logger.error(f\"Sagemaker Runtime error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unknown error: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"Endpoint {endpoint_name} is not ready to be queried (status={endpoint_status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519668d",
   "metadata": {},
   "source": [
    "---\n",
    "## User-defined values\n",
    "\n",
    "Edit the values below to match your own development environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_S3_BUCKET = \"mistral-development\"\n",
    "USER_S3_DATA_DIR = \"harizo-tests/\"\n",
    "USER_SAGEMAKER_ALGORITHM_ARN = \"arn:aws:sagemaker:us-west-2:777356365391:algorithm/mistral-finetune-for-ministral-3b-2410-1733402468\"\n",
    "USER_SAGEMAKER_TRAINING_INSTANCE_TYPE = \"ml.p5.48xlarge\"\n",
    "USER_SAGEMAKER_INFERENCE_INSTANCE_TYPE = \"ml.p4d.24xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f001ff7",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> **IMPORTANT**: This notebook must run on a SageMaker Notebook instance to directly leverage the underlying execution role for performing actions and reading data on your AWS tenant.\n",
    "\n",
    "### Data\n",
    "\n",
    "The SageMaker fine-tuning capabilities rely on the AWS S3 service to host the input data and output artifact files.\n",
    "\n",
    "The rest of this notebook will assume that your data resides in a S3 bucket (`USER_S3_BUCKET`) within a specified directory (`USER_S3_DATA_DIR`) following this layout:\n",
    "```\n",
    ".\n",
    "|_$USER_S3_DATA_DIR\n",
    "    |_config.yml\n",
    "    |_training.jsonl\n",
    "    |_validation.jsonl\n",
    "    |_lora_adapters/\n",
    "```\n",
    "\n",
    "A brief description of these items is provided below.\n",
    "\n",
    "#### Training Data & Configuration Files\n",
    "\n",
    "To fine-tune a Mistral model, the key ingredient you need is training data, split in two distinct files:\n",
    "\n",
    "- `training.jsonl`: Contains the records based on which the model will update its weights.\n",
    "- `validation.jsonl`: Contains a smaller set of records used to assess the model's performance at a given time during training.\n",
    "\n",
    "In both files, each record follows the same structure as described in the [Mistral documentation](https://docs.mistral.ai/capabilities/finetuning/). Examples of training data files are available in the `data/` directory.\n",
    "\n",
    "The `config.yml` configuration file contains additional settings specific to the Mistral fine-tuning codebase.\n",
    "\n",
    "#### LoRA Adapters Directory\n",
    "\n",
    "The result of a fine-tuning algorithm run is a small artifact called a _LoRA adapter_. When you deploy your fine-tuned model, you first create a _model package_. During this process, the LoRA adapter is merged into the base model to create a new variant that can then be deployed as a regular endpoint. When running a training job, you need to specify a target directory where adapters will be stored. In your case, this directory will be `lora_adapters/`. \n",
    "\n",
    "\n",
    "### IAM permissions\n",
    "\n",
    "To run this example end-to-end, the AWS IAM role you will use requires the following permissions:\n",
    "\n",
    "- All permissions from the `AmazonSageMakerFullAccess` role\n",
    "- Authority to make AWS Marketplace subscriptions in the AWS account used:\n",
    "  - `aws-marketplace:ViewSubscriptions`\n",
    "  - `aws-marketplace:Unsubscribe`\n",
    "  - `aws-marketplace:Subscribe`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d192913",
   "metadata": {},
   "source": [
    "## Create and start a fine-tuning job \n",
    "\n",
    "The following function creates a training job to fine-tune the base model from `USER_SAGEMAKER_ALGORITHM_ARN` using the `training.jsonl` and `validation.jsonl` datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_params = {\n",
    "    \"execution_role_arn\": sage.get_execution_role(),\n",
    "    \"algorithm_arn\": USER_SAGEMAKER_ALGORITHM_ARN,\n",
    "    \"training_dataset\": build_s3_uri(USER_S3_BUCKET, USER_S3_DATA_DIR, \"training.jsonl\"),\n",
    "    \"validation_dataset\": build_s3_uri(USER_S3_BUCKET, USER_S3_DATA_DIR, \"validation.jsonl\"),\n",
    "    \"config_file\": build_s3_uri(USER_S3_BUCKET, USER_S3_DATA_DIR, \"config.yaml\"),\n",
    "    \"lora_dir\": build_s3_uri(USER_S3_BUCKET, USER_S3_DATA_DIR, \"lora_adapters/\"),\n",
    "    \"instance_type\": USER_SAGEMAKER_TRAINING_INSTANCE_TYPE\n",
    "}\n",
    "\n",
    "ft_job_arn = create_mistral_ft_job(job_params=job_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bfdea",
   "metadata": {},
   "source": [
    "Once the training job has started, you can get its status and summary information with this function, which can directly reuse the output of `create_mistral_ft_job()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_job_info = get_mistral_ft_job_info(ft_job_arn)\n",
    "print((ft_job_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfe7ef",
   "metadata": {},
   "source": [
    "## Deploy your fine-tuned model endpoint\n",
    "\n",
    "Once your fine-tuning job has successfully completed, you will need to complete a few steps before using the resulting model.\n",
    "\n",
    "### Create a model package\n",
    "\n",
    "First, you need to make the model deployable by creating a _model package resource_. LoRA adapters cannot be directly deployed, instead they have to be recombined with the base model to create an operational model variant.\n",
    "The following operation will execute these steps and trigger the creation of a model package resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = create_mistral_ft_model_package(ft_job_info=ft_job_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3add9",
   "metadata": {},
   "source": [
    "### Deploy the endpoint\n",
    "\n",
    "Next, you need to provision the resources required to host your endpoint, then deploy the endpoint. This is a 3-step process where you create distinct resources:\n",
    "\n",
    "1. A _model_ which contains the model artifacts and the inference code\n",
    "2. An _endpoint configuration_ where you specify the configuration details for deploying your model (e.g. the instance type\n",
    "3. A _model endpoint_ which is the actual instance of the model that serves inference requests.\n",
    "\n",
    "The `create_mistral_ft_endpoint()` function combines all these steps into a single entrypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92352446",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_arn = create_mistral_ft_endpoint(model_package_arn=model_package,\n",
    "                           instance_type=USER_SAGEMAKER_INFERENCE_INSTANCE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb5919",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "\n",
    "Once your endpoint is up and running, you can test it by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_arn = 'arn:aws:sagemaker:us-west-2:777356365391:endpoint/ministral-3b-2410-ft-202412271035-model'\n",
    "messages = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Who is the best French painter? Answer in one short sentence.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"model\": \"ft:model\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0,\n",
    "    \"stream\": False\n",
    "})\n",
    "\n",
    "inference_resp = query_mistral_ft_endpoint(endpoint_arn=endpoint_arn,\n",
    "                                           payload=payload)\n",
    "\n",
    "print(inference_resp[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b818d6",
   "metadata": {},
   "source": [
    "Congratulations, you have completed an end-to-end training and deployment of a fine-tuned Mistral model on AWS Sagemaker!\n",
    "\n",
    "## Clean-up\n",
    "\n",
    "Once you have finished your inference tests, you should delete the deployed endpoint to avoid excessive infrastructure charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056221d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Delete endpoint\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "\n",
    "endpoint_name = endpoint_arn.split('/')[-1]\n",
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba35f1f",
   "metadata": {},
   "source": [
    "You can also optionally delete the other resources created when executing this notebook:\n",
    "- the endpoint configuration,\n",
    "- the model,\n",
    "- the model package,\n",
    "- the model artifact in your S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b66a4",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "Here are some useful links to better understand some of the topics introduced in this notebook:\n",
    "\n",
    "- [boto3 reference for Sagemaker](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html)\n",
    "- [boto3 reference for Sagemaker Runtime (inference)](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)\n",
    "- [Mistral AI reference documentation for the chat completion API](https://docs.mistral.ai/api/#tag/chat/operation/chat_completion_v1_chat_completions_post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
