{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430812bf-9c4e-455b-803d-880bf67d9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import httpx\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from typing import Dict, Any\n",
    "from sagemaker import ModelPackage\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0ab7f-6239-4035-a7cd-9e90f41233d5",
   "metadata": {},
   "source": [
    "# Deploying and running Mistral OCR on SageMaker\n",
    "\n",
    "This notebook contains the basic steps to deploy and run the Mistral OCR model on SageMaker.\n",
    "\n",
    "> **Pre-requisites**\n",
    "> - The ARN of the Mistral OCR model package\n",
    "> - An IAM role with sufficient permissions to deploy a SageMaker endpoint and run inference on it\n",
    "\n",
    "You can run this notebook anywhere (not necessarily in SageMaker) provided that you have properly configured the AWS authentication in your development environment.\n",
    "\n",
    "You will need to fill in all the following parameters in the code cell below to run this notebook:\n",
    "\n",
    "| Name | Description |\n",
    "| --- | --- |\n",
    "| `MISTRAL_OCR_MODEL_PACKAGE_ARN` | The ARN of the Mistral OCR model package |\n",
    "| `MISTRAL_OCR_MODEL_CONFIG_INSTANCE_TYPE` | The EC2 instance type used to host the endpoint (e.g. `ml.g6.24xlarge`) |\n",
    "| `SAGEMAKER_EXECUTION_ROLE_ARN` | The ARN of the IAM role with permissions to deploy and run SageMaker endpoints |\n",
    "| `MISTRAL_OCR_ENDPOINT_NAME` | The name you want to give to your endpoint |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722d920-0712-469d-b05f-bca43fb6a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_OCR_MODEL_PACKAGE_ARN = \"\" \n",
    "MISTRAL_OCR_MODEL_CONFIG_INSTANCE_TYPE = \"\" \n",
    "SAGEMAKER_EXECUTION_ROLE_ARN = \"\" \n",
    "MISTRAL_OCR_ENDPOINT_NAME = \"\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d0f9c-bca0-4f60-a0fe-5bce225c03cd",
   "metadata": {},
   "source": [
    "## 1. Deploy the model\n",
    "\n",
    "Start by defining the `deploy_sagemaker_model()` function that will start an endpoint which we will query for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77afed9-ff5d-4014-b0bb-1d10a5f01aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_sagemaker_model(model_package_arn: str, execution_role: str, instance_type: str, endpoint_name: str,):\n",
    "    session = sagemaker.Session()\n",
    "    role = execution_role\n",
    "    model_package = ModelPackage(\n",
    "        role=role,\n",
    "        model_package_arn=model_package_arn,\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "    model_package.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name,\n",
    "        model_data_download_timeout=3600,\n",
    "        container_startup_health_check_timeout=3600\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a4e53-c907-4ae5-a0fc-b7430ebdc5a1",
   "metadata": {},
   "source": [
    "Then run this function with the parameters of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf99706-8212-46f2-ab6a-33651420266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_sagemaker_model(\n",
    "    model_package_arn=MISTRAL_OCR_MODEL_PACKAGE_ARN,\n",
    "    execution_role=SAGEMAKER_EXECUTION_ROLE_ARN,\n",
    "    instance_type=MISTRAL_OCR_MODEL_CONFIG_INSTANCE_TYPE,\n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78635336-7238-49e8-829e-de94fdaed2e2",
   "metadata": {},
   "source": [
    "## 2. Run inference on the model\n",
    "\n",
    "Now that the endpoint is live you can start querying it. The OCR API differs quite a bit from the usual chat completion API, so make sure to familiarize yourself with its specifics by reading the [documentation](https://docs.mistral.ai/api/#tag/ocr/operation/ocr_v1_ocr_post) before moving forward.\n",
    "\n",
    "Start by defining a helper function `run_inference()` to wrap the model invocation into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0760eb-42c8-4c6b-8f41-8fd5d810b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(client, endpoint_name: str, payload: dict[str,Any]) -> Dict[str, Any]:\n",
    "    try:\n",
    "        inference_out = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "        return json.loads(inference_resp_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a499a-5872-4755-aea9-d626ef90f797",
   "metadata": {},
   "source": [
    "### 2.1 With URL inputs\n",
    "\n",
    "The following example runs the entire [Pixtral technical report](https://arxiv.org/pdf/2410.07073) through the OCR model. The input is listed as `document_url`, meaning that the input payload contains a link pointing to the document to be downloaded and parsed. For brevity, we only scan the first page by passing `\"pages\": [0]\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a0435-56f2-4604-93c2-10793dcf762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "pixtral_report_pdf_url = \"https://arxiv.org/pdf/2410.07073\"\n",
    "payload ={\n",
    "    \"model\": \"mistral-ocr-2503\",\n",
    "    \"document\": {\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": pixtral_report_pdf_url\n",
    "\n",
    "    },\n",
    "    \"pages\": \"0\"\n",
    "}\n",
    "pixtral_report_parsed = run_inference(client=client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "Markdown(pixtral_report_parsed[\"pages\"][0][\"markdown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59c6ed-df65-4677-877b-d239f5b3d551",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> You can also extract images from documents by setting the `include_image_base64` flag to `True` in the input payload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7352ae5-d50b-4bd9-99b2-43e5d7119ee8",
   "metadata": {},
   "source": [
    "### 2.2 With base64-encoded inputs\n",
    "\n",
    "The OCR API endpoint also accepts base64-encoded inputs. This is particularly useful when your documents are stored locally.\n",
    "\n",
    "The following example encodes an image taken from the [Mistral AI blog](https://mistral.ai/fr/news/pixtral-large) before passing it to the endpoint. To build a base64-encoded image example from an image URL, you will use the followign function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e40323-d7a9-47d9-9ffe-a5de3899eb2c",
   "metadata": {},
   "source": [
    "You can now call the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b462211-bff6-47a2-8fcf-c572bd802e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_encode_image(url: str) -> str:\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(url, timeout=10)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        # Encode the image content to base64\n",
    "        image_data = response.content\n",
    "        base64_encoded_data = base64.b64encode(image_data).decode('utf-8')\n",
    "        return base64_encoded_data\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {exc.request.url}\")\n",
    "        raise\n",
    "    except httpx.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caad79-217d-4a5f-b82d-e27ae2c0c078",
   "metadata": {},
   "source": [
    "Note that if you want to test the endpoint with your own images, you can discard the function above and directly pass in your data.\n",
    "\n",
    "Run the OCR API call as follows, and notice how the `\"type\"` has changed to `\"image_url`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95445176-d651-4b43-840d-0bd5cc418cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt_image_url = \"https://cms.mistral.ai/assets/1d7df1b8-5caa-47b9-b6a1-666b05d38019\"\n",
    "receipt_image_b64 = download_and_encode_image(url=receipt_image_url)\n",
    "\n",
    "payload ={\n",
    "    \"model\": \"mistral-ocr-2503\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{receipt_image_b64}\"\n",
    "    }\n",
    "}\n",
    "receipt_parsed = run_inference(client=client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "Markdown(receipt_parsed[\"pages\"][0][\"markdown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a204c-edd6-4885-b329-a8af5ce3d1be",
   "metadata": {},
   "source": [
    "## 3. Cleanup\n",
    "\n",
    "Once you don't need the endpoint anymore, you should delete it to avoid unnecessary resource consumption. Since several resource types were created when the endpoint was spawned, here is the deletion sequence you should follow:\n",
    "\n",
    "1. Delete the endpoint\n",
    "2. Delete the endpoint configuration\n",
    "3. Delete the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c894f2-7b8b-4007-a603-8fc889e3425b",
   "metadata": {},
   "source": [
    "## 4. Wrapping up\n",
    "\n",
    "Here are some useful resources to further explore the topic of the Mistral OCR model:\n",
    "\n",
    "- [Mistral blog post on the OCR API release](https://mistral.ai/fr/news/mistral-ocr)\n",
    "- [Mistral documentation on OCR capabilities](https://docs.mistral.ai/capabilities/document/)\n",
    "- [Mistral OCR API reference documentation](https://docs.mistral.ai/capabilities/document/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
